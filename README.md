# Reflector Position Optimization

Physics-aware optimal placement for mechanical reflectors in NLOS (Non-Line-of-Sight) scenarios using differentiable ray tracing with Sionna.

> ðŸ“Š **Project Status**: See [STATUS.md](STATUS.md) for current development status, roadmap, and completed features.

## Features

- **Grid Search Optimization**: Exhaustive search over spatial grid for baseline performance
- **Gradient Descent Optimization**: Fast gradient-based optimization using differentiable ray tracing
- **Genetic Algorithm (DEAP)**: Evolutionary optimization with population-based search using the DEAP library
- **Reflector Initialization & Control**: Mechanical reflector setup and runtime control integrated into scene/optimizer flows
- **Ray-Parallel Execution**: Distributed evaluation via Ray ActorPool â€” all three methods run in parallel across persistent GPU workers
- **Validated Execution Paths**: Verified runs with and without Ray, including parallel multi-GPU execution
- **Inversion of Control (IoC) Architecture**: Clean separation of algorithm logic (DEAP) from execution engine (Ray) via dependency injection
- **Metrics**: Minimum RSS, coverage area, and soft minimum for smooth optimization
- **Visualizations**: Heatmaps, convergence plots, trajectory visualization, and GA evolution plots
- **CLI Tool**: Command-line interface for easy experimentation

## Installation

### From Source

```bash
# Clone the repository
cd reflector-position

# Create virtual environment
python3 -m venv .venv
source .venv/bin/activate

# Install the package
pip install -e .

# Or install with development dependencies
pip install -e ".[dev]"
```

### Requirements

- Python >= 3.10, < 3.14
- TensorFlow >= 2.20.0
- Sionna >= 1.2.1
- PyTorch >= 2.9.0
- DrJit >= 1.2.0
- Mitsuba >= 3.7.0
- NumPy == 1.26.4

## Quick Start

### Command Line Interface

The package provides a CLI tool `reflector-optimize`:

```bash
# Run gradient descent only
reflector-optimize /path/to/scene.xml --method gradient-descent

# Run grid search only
reflector-optimize /path/to/scene.xml --method grid-search

# Run both methods and compare
reflector-optimize /path/to/scene.xml --method all

# Customize parameters
reflector-optimize /path/to/scene.xml \
    --method gradient-descent \
    --gd-iterations 20 \
    --gd-lr 0.5 \
    --gd-samples 1000000
```

#### CLI Options

**Method Selection:**
- `--method {grid-search,gradient-descent,all}`: Choose optimization method

**Scene Configuration:**
- `--frequency FLOAT`: Operating frequency in Hz (default: 5.18e9)
- `--tx-power FLOAT`: Transmitter power in dBm (default: 5.0)
- `--fixed-z FLOAT`: Fixed Z height for AP (default: 3.8)

**Grid Search Options:**
- `--gs-x-min`, `--gs-x-max`: X bounds for grid search
- `--gs-y-min`, `--gs-y-max`: Y bounds for grid search
- `--gs-resolution FLOAT`: Grid spacing in meters (default: 5.0)
- `--gs-samples INT`: Ray tracing samples (default: 500000)
- `--gs-max-depth INT`: Max ray tracing depth (default: 13)

**Gradient Descent Options:**
- `--gd-init-x`, `--gd-init-y`: Initial position
- `--gd-x-min`, `--gd-x-max`: X bounds
- `--gd-y-min`, `--gd-y-max`: Y bounds
- `--gd-iterations INT`: Number of iterations (default: 10)
- `--gd-lr FLOAT`: Learning rate (default: 0.5)
- `--gd-samples INT`: Ray tracing samples (default: 1000000)
- `--gd-max-depth INT`: Max ray tracing depth (default: 15)
- `--gd-temperature FLOAT`: Soft minimum temperature (default: 0.2)

**Other Options:**
- `--quiet`: Suppress verbose output

### Python API

#### Quick Example

```python
from reflector_position import (
    setup_building_floor_scene,
    GradientDescentAPOptimizer,
    GradientDescentConfig,
)

# Setup scene
scene = setup_building_floor_scene(
    scene_path="/path/to/scene.xml",
    frequency=5.18e9,
    tx_power_dbm=5.0,
)

# Configure optimizer
config = GradientDescentConfig(
    initial_x=20.0,
    initial_y=20.0,
    num_iterations=10,
    learning_rate=0.5,
    samples_per_tx=1_000_000,
)

# Run optimization
optimizer = GradientDescentAPOptimizer(
    scene=scene,
    initial_position=config.initial_position,
    position_bounds=config.position_bounds,
)

final_position, final_rss = optimizer.optimize(
    num_iterations=config.num_iterations,
    learning_rate=config.learning_rate,
    samples_per_tx=config.samples_per_tx,
)

# Visualize results
optimizer.plot_optimization_trajectory()
```

#### Grid Search Example

```python
from reflector_position import GridSearchAPOptimizer, GridSearchConfig

# Configure grid search
config = GridSearchConfig(
    x_min=5.0,
    x_max=35.0,
    y_min=5.0,
    y_max=35.0,
    grid_resolution=2.0,
    samples_per_tx=500_000,
)

# Run optimization
optimizer = GridSearchAPOptimizer(
    scene=scene,
    search_bounds=config.search_bounds,
    grid_resolution=config.grid_resolution,
)

best_position, best_rss = optimizer.optimize()

# Visualize results
optimizer.plot_results(metric='min_rss_dbm')
```

## Examples

See the `examples/` directory for complete examples:

- `examples/quick_test.py`: Fast gradient descent test with reduced parameters
- `examples/full_comparison.py`: Compare grid search vs gradient descent
- `examples/ray_parallel_example.py`: Ray-parallel gradient descent (64 tasks) and grid search (441 points) via ActorPool
- `examples/run_ga_modular.py`: **Modular GA** â€” DEAP genetic algorithm with Ray-parallel fitness evaluation (IoC pattern)
- `examples/ray_experiment_runner.py`: Unified Ray runner for hyperparameter sweeps across GD / GS / GA (one method per trial)

Run examples:

```bash
python examples/quick_test.py
python examples/full_comparison.py
python examples/ray_parallel_example.py
python examples/run_ga_modular.py
python examples/ray_experiment_runner.py --config examples/ray_experiment_runner_config.example.json
```

## Ray Runner (Unified)

`examples/ray_experiment_runner.py` is the single entrypoint for Ray-based hyperparameter automation.

- Runs one method per trial (`gd`, `gs`, or `ga`), avoiding repeated execution of unrelated methods
- Supports explicit trials and automatic hyperparameter sweeps via `sweep_groups`
- Saves per-trial logs and consolidated summaries (`summary.csv`, `summary.json`, `all_trials_detailed.json`)

### Step 1: Generate explicit trial config (recommended)

```bash
python examples/ray_experiment_runner.py \
    --config examples/ray_experiment_runner_config.example.json \
    --generate-only \
    --generated-config results/generated_trials.json
```

### Step 2: Run hyperparameter optimization

Using the original config:

```bash
python examples/ray_experiment_runner.py \
    --config examples/ray_experiment_runner_config.example.json \
    --output-root results/experiments
```

Using the generated explicit trial config:

```bash
python examples/ray_experiment_runner.py \
    --config results/generated_trials.json \
    --output-root results/experiments
```

### Output location

Each run is stored under:

- `results/experiments/ray_experiments_<timestamp>/`

For full configuration details and tuning patterns, see [docs/guides/RAY_EXPERIMENT_RUNNER.md](docs/guides/RAY_EXPERIMENT_RUNNER.md).

## Project Structure

```
reflector-position/
â”œâ”€â”€ src/reflector_position/
â”‚   â”œâ”€â”€ __init__.py              # Package initialization
â”‚   â”œâ”€â”€ cli.py                   # Command-line interface
â”‚   â”œâ”€â”€ config.py                # Configuration dataclasses
â”‚   â”œâ”€â”€ metrics.py               # RSS metrics and utilities
â”‚   â”œâ”€â”€ scene_setup.py           # Scene configuration
â”‚   â”œâ”€â”€ utils.py                 # Helper functions
â”‚   â””â”€â”€ optimizers/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ base_optimizer.py        # Abstract base class
â”‚       â”œâ”€â”€ gradient_descent.py      # Gradient descent (differentiable RT)
â”‚       â”œâ”€â”€ grid_search.py           # Grid search + SinglePointGridSearch
â”‚       â”œâ”€â”€ optimizer_factory.py     # Factory pattern for optimizer creation
â”‚       â”œâ”€â”€ ray_parallel_optimizer.py # ActorPool orchestrator + OptimizationWorker
â”‚       â”œâ”€â”€ ray_evaluator.py         # Generic Ray execution engine (IoC)
â”‚       â”œâ”€â”€ deap_logic.py            # Pure DEAP GA logic (no Ray imports)
â”‚       â””â”€â”€ ray_deap_optimizer.py    # Monolithic DEAP+Ray (legacy)
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ ray_parallel_example.py  # Parallel GD + GS via ActorPool
â”‚   â”œâ”€â”€ run_ga_modular.py        # Modular GA entry point (IoC)
â”‚   â””â”€â”€ ...                      # Other examples
â”œâ”€â”€ docs/                        # Comprehensive documentation
â”œâ”€â”€ tests/                       # pytest test suite (62 tests)
â”œâ”€â”€ pyproject.toml               # Package configuration
â””â”€â”€ README.md                    # This file
```

## Methodology

### Overview

The framework implements **physics-aware optimization** using differentiable ray tracing, enabling gradient-based methods that understand the physical propagation environment. For advanced Ray-based distributed optimization and baseline comparisons, see the detailed [methodology documentation](docs/methodology/).

### Optimization Approaches

#### Grid Search

Exhaustively evaluates AP positions on a 2D grid:
- Computes radio map for each position
- Tracks minimum RSS across coverage area
- Serves as baseline for comparison

#### Gradient Descent (Differentiable Ray Tracing)

Uses differentiable ray tracing to optimize via gradients:
- Leverages Sionna's differentiable RadioMapSolver
- Uses soft minimum (LogSumExp) for smooth gradients
- PyTorch + DrJit integration via `@dr.wrap` decorator
- 50-100Ã— faster than grid search

#### Genetic Algorithm (DEAP Library) âœ…

Evolutionary optimisation using the DEAP framework:
- **Population-based search**: 50-100 individuals encoding (x, y) AP positions
- **Operators**: Blend crossover (`cxBlend`), Gaussian mutation, tournament selection
- **Maximises minimum RSS** (linear Watts) as fitness
- **Ray-parallel evaluation**: each individual evaluated via `SinglePointGridSearchOptimizer` on Ray ActorPool
- **Modular IoC architecture**: algorithm logic (no Ray imports) separated from execution engine

### Ray-Based Distributed Optimization âœ…

All three methods (GD, GS, GA) run on a shared **Ray ActorPool** infrastructure:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              Driver (Algorithm Logic)                    â”‚
â”‚  Gradient Descent / Grid Search / DEAP GA               â”‚
â”‚         â”‚                                               â”‚
â”‚         â”‚  toolbox.map(evaluate, population)             â”‚
â”‚         â–¼                                               â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                        â”‚
â”‚  â”‚  RayActorPoolExecutor.map() â”‚  â† Dependency Injectionâ”‚
â”‚  â”‚  (pool.map â€” ordered, sync) â”‚                        â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                        â”‚
â”‚             â”‚                                           â”‚
â”‚    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”                         â”‚
â”‚    â–¼        â–¼        â–¼        â–¼                         â”‚
â”‚  Worker0  Worker1  Worker2  Worker3                     â”‚
â”‚  (Scene)  (Scene)  (Scene)  (Scene)                     â”‚
â”‚  GPU 0.25 GPU 0.25 GPU 0.25 GPU 0.25                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Key Features:**
- **ActorPool pattern**: Fixed pool of persistent workers; Scene loaded once per worker
- **Ordered synchronous map** (`pool.map`): prevents freeze issues from `map_unordered`
- **GPU efficiency**: Configurable fraction per worker (0.25 = 4 workers/GPU)
- **IoC pattern**: Algorithm logic knows nothing about Ray; uses injected `map` function
- **Three optimisation methods**: GD (multi-start), GS (true parallel), GA (DEAP evolutionary)

**Quick Example â€” Modular GA:**
```python
import ray
from reflector_position.optimizers import RayActorPoolExecutor, GeneticAlgorithmRunner

ray.init()
executor = RayActorPoolExecutor(scene_config={...}, num_workers=4, gpu_fraction=0.25)

ga = GeneticAlgorithmRunner(
    position_bounds={"x_min": 5, "x_max": 25, "y_min": 5, "y_max": 25},
    fixed_z=3.8,
    executor_map=executor.map,  # Dependency Injection
)

results = ga.run(
    optimization_params={"samples_per_tx": 1_000_000, "max_depth": 13},
    ga_params={"pop_size": 50, "n_gen": 20},
    seed=42,
)

print(f"Best: {results['best_position']}  RSS: {results['best_fitness_dbm']:.2f} dBm")
executor.shutdown()
```

**For complete details**, see:
- [RAY_PARALLEL_GUIDE.md](docs/methodology/RAY_PARALLEL_GUIDE.md) - Complete guide with examples
- [RAY_ARCHITECTURE.md](docs/methodology/RAY_ARCHITECTURE.md) - Why Ray vs vectorization
- [OPTIMIZATION_WORKFLOW.md](docs/methodology/OPTIMIZATION_WORKFLOW.md) - Complete architecture
- [RAY_IMPLEMENTATION_SUMMARY.md](docs/methodology/RAY_IMPLEMENTATION_SUMMARY.md) - Implementation status
- [BASELINES.md](docs/methodology/BASELINES.md) - Comparison with GA, PSO, and Alternating Optimization

### Metrics

- **Minimum RSS**: Worst-case received signal strength (optimization objective)
- **Soft Minimum**: Differentiable approximation using LogSumExp
- **Coverage**: Percentage of area above threshold (-100 dBm default)


## Performance

Typical performance on building floor scenario:

| Method | Evaluations | Time | Solution Quality |
|--------|-------------|------|------------------|
| Grid Search (2m grid) | ~100-200 | ~30-60 min | Baseline |
| Gradient Descent | 10-20 | ~20-40 min | Within 1 dB |
| Ray Parallel GD (4 workers, 64 tasks) | 640 | ~10-20 min | Best of 64 starts |
| Ray Parallel GS (4 workers, 441 pts) | 441 | ~5-15 min | Exhaustive |
| DEAP GA (pop=50, 20 gen, 4 workers) | ~700-1000 | ~15-30 min | Population-optimal |

Gradient descent achieves similar quality with significantly fewer evaluations. The DEAP GA explores the search space more broadly via an evolving population, complementing gradient-based methods. All three methods leverage the same Ray ActorPool for parallel evaluation with near-linear speedup.

**Testing Status**: Core optimizers validated with 62 unit and integration tests (82-92% coverage). Ray parallel and GA implementations complete.

## Development

### Setup Development Environment

```bash
# Install with dev dependencies
pip install -e ".[dev]"

# Format code
black src/ examples/

# Lint code
ruff check src/ examples/

# Run type checking
mypy src/
```

### Running Tests

```bash
pytest
```

## Completed Features âœ…

### Core Functionality
- âœ… **Grid Search Optimizer**: Exhaustive spatial search with configurable resolution
- âœ… **Gradient Descent Optimizer**: Differentiable ray tracing with PyTorch + DrJit
- âœ… **Soft Minimum Metric**: Smooth, differentiable optimization objective
- âœ… **Coverage Metrics**: RSS threshold-based coverage calculation
- âœ… **Radio Map Computation**: Configurable ray tracing parameters
- âœ… **Reflector Initialization & Control**: Reflector-aware scene setup and runtime control integrated in optimization flow
- âœ… **Optimizer Factory**: Factory pattern for creating optimizers
- âœ… **Base Optimizer ABC**: Abstract base class enforcing optimizer interface

### Ray-Based Parallel Optimization âœ…
- âœ… **RayParallelOptimizer**: ActorPool orchestrator for distributed multi-start GD and parallel GS
- âœ… **OptimizationWorker**: Persistent Ray actor with reusable Scene instance
- âœ… **RayActorPoolExecutor**: Generic execution engine with ordered `pool.map` (IoC pattern)
- âœ… **GeneticAlgorithmRunner**: Pure DEAP GA logic â€” no Ray imports, uses injected `map`
- âœ… **SinglePointGridSearchOptimizer**: Evaluates single (x, y) position for GA fitness
- âœ… **GPU Management**: Configurable GPU fraction per worker (0.25 = 4 workers/GPU)
- âœ… **Multi-GPU Validation**: Parallel Ray execution validated on multi-GPU setup
- âœ… **Non-Ray Validation**: Baseline single-process execution validated for reflector-aware paths
- âœ… **Freeze-safe**: Uses `pool.map` (ordered, synchronous) instead of `map_unordered`
- âœ… **Comprehensive Documentation**: Detailed guides with examples

### Testing & Quality Assurance âœ…
- âœ… **Unit Tests**: 62 tests across 4 test files
- âœ… **Test Coverage**: 82-92% core coverage, 100% factory coverage
- âœ… **Test Framework**: pytest with markers (unit, integration, slow)
- âœ… **Shared Fixtures**: Efficient scene setup and reuse
- âœ… **Fast Execution**: ~10s for full test suite
- âœ… **Test Documentation**: Comprehensive guides in `docs/tests/`

### Code Architecture
- âœ… **Modular Design**: Separate modules for metrics, optimizers, config, and scene setup
- âœ… **Type Hints**: Full type annotations across all public APIs
- âœ… **Configuration System**: Type-safe dataclasses for all parameters
- âœ… **Error Handling**: Input validation and clear error messages
- âœ… **Factory Pattern**: Extensible optimizer creation system

### User Interface
- âœ… **CLI Tool**: `reflector-optimize` command-line interface
- âœ… **Python API**: Clean, documented API for programmatic use
- âœ… **Visualization**: Heatmaps, trajectory plots, convergence graphs
- âœ… **Examples**: Quick test, full comparison, Ray parallel examples

### Documentation
- âœ… **README**: Main documentation with quick start
- âœ… **Installation Guide**: Detailed setup instructions (`docs/guides/INSTALL.md`)
- âœ… **Usage Guide**: Comprehensive examples (`docs/guides/USAGE.md`)
- âœ… **Quick Reference**: Cheat sheet (`docs/guides/QUICKREF.md`)
- âœ… **Project Structure**: Architecture documentation (`docs/architecture/PROJECT_STRUCTURE.md`)
- âœ… **Changelog**: Migration details (`docs/architecture/CHANGELOG.md`)
- âœ… **Ray Parallel Guide**: Complete guide with examples (`docs/methodology/RAY_PARALLEL_GUIDE.md`)
- âœ… **Ray Architecture**: Why Ray vs vectorization (`docs/methodology/RAY_ARCHITECTURE.md`)
- âœ… **Test Documentation**: Testing guides and summaries (`docs/tests/`)
- âœ… **Methodology**: Optimization workflow and baselines

### Package Management
- âœ… **pyproject.toml**: Modern Python packaging
- âœ… **Entry Points**: CLI command installation
- âœ… **Dependencies**: Pinned versions for reproducibility (including Ray)
- âœ… **Editable Install**: Development-friendly installation

## TODO & Future Enhancements ðŸš€

### High Priority
- [x] **Unit Tests**: Add pytest test suite for core functionality âœ…
  - [x] Test metrics calculations âœ…
  - [x] Test optimizer convergence âœ…
  - [x] Test scene setup utilities âœ…
  - [x] Test configuration validation âœ…
  - [x] Test factory pattern âœ…
  - [x] Test base optimizer ABC âœ…
- [x] **Integration Tests**: End-to-end optimization tests âœ…
- [ ] **Ray Parallel Tests**: Unit and integration tests for Ray implementation
  - [ ] Test RayParallelOptimizer initialization and configuration
  - [ ] Test OptimizationWorker spawning and lifecycle
  - [ ] Test result aggregation and winner selection
  - [ ] Test GPU fraction allocation
  - [ ] Test error handling and recovery
- [ ] **CLI Tests**: Test command-line interface functionality
- [ ] **CI/CD Pipeline**: GitHub Actions for automated testing
- [ ] **Type Checking**: Add mypy to CI pipeline

### Performance Improvements
- [x] **Ray Distributed Optimization**: Implement multi-process optimization for reflector positioning âœ…
  - [x] RayParallelOptimizer with configurable workers âœ…
  - [x] OptimizationWorker with process isolation âœ…
  - [x] GPU fraction management âœ…
  - [ ] Testing and validation â³
- [x] **GPU Memory Management**: Configurable VRAM usage per worker âœ…
- [ ] **Caching**: Cache radio maps for repeated positions
- [ ] **Memory Optimization**: Reduce memory footprint for large scenes
- [ ] **Async Result Collection**: Stream results as they complete
- [ ] **Checkpointing**: Save intermediate results for long-running optimizations

### New Features
- [ ] **Multi-Objective Optimization**: Simultaneous coverage + capacity optimization
- [ ] **Constrained Optimization**: Wall-mounting and mechanical constraints
- [x] **Reflector Control**: Mechanical reflector initialization and control integrated into main optimization path âœ…
- [ ] **Multi-AP Optimization**: Joint optimization of multiple access points
- [ ] **Different Environments**: Support for corridor, warehouse, outdoor scenes
- [ ] **Adaptive Learning Rate**: Automatic learning rate scheduling
- [ ] **Early Stopping**: Convergence detection

### Visualization & Analysis
- [ ] **Interactive Plots**: Plotly/Bokeh integration for interactive exploration
- [ ] **3D Visualization**: 3D scene rendering with radio coverage
- [ ] **Animation**: Animated optimization trajectory
- [ ] **Comparative Analysis**: Automated method comparison reports
- [ ] **Heat Map Export**: Save results as images/videos

### Documentation
- [ ] **API Documentation**: Sphinx-generated API reference
- [ ] **Tutorials**: Step-by-step tutorials for different scenarios
- [ ] **Jupyter Notebooks**: Interactive tutorial notebooks
- [ ] **Performance Benchmarks**: Benchmark results for different configurations
- [ ] **Video Demos**: Screen recordings demonstrating usage

### Code Quality
- [ ] **Linting**: Enforce code style with ruff/black in pre-commit hooks
- [ ] **Code Coverage**: Aim for >80% test coverage
- [ ] **Security Scanning**: Add dependency vulnerability scanning
- [ ] **Documentation Coverage**: Ensure all public APIs are documented

### Publishing & Distribution
- [ ] **PyPI Release**: Publish package to PyPI
- [ ] **Docker Image**: Containerized version with all dependencies
- [ ] **Conda Package**: conda-forge distribution
- [ ] **Documentation Site**: GitHub Pages or Read the Docs hosting
- [ ] **Zenodo DOI**: Citable version with DOI

### Research Extensions
- [ ] **Phase 2**: Joint AP + single RIS optimization (from roadmap)
- [ ] **Phase 3**: Multi-AP, multi-RIS optimization (from roadmap)
- [ ] **Discontinuity Smoothing**: Sigmoid-based smoothing (Eertmans et al.)
- [ ] **Learned Schedules**: Machine learning for hyperparameter tuning
- [ ] **Real-World Validation**: Experimental validation with measurements

### User Experience
- [ ] **Progress Bars**: tqdm integration for long-running optimizations
- [ ] **Resume Capability**: Save/load optimization state
- [ ] **Configuration Files**: YAML/JSON config file support
- [ ] **Logging**: Structured logging with different verbosity levels
- [ ] **Result Export**: Export results to JSON, CSV, HDF5

## Roadmap

### Phase 1: Core Functionality âœ… COMPLETE
- Grid search baseline
- Gradient descent optimization
- Basic visualization
- Package structure
- Documentation
- Unit and integration tests (62 tests, 82-92% coverage)
- Factory pattern and base classes

**Status**: âœ… Complete (January 2026)

### Phase 2: Ray-Based Parallel Optimization âœ… COMPLETE
- Ray distributed architecture (ActorPool pattern)
- Multi-start gradient descent (64 tasks â†’ 4 workers)
- True parallel grid search (441 single-point tasks)
- DEAP genetic algorithm with Ray-parallel fitness evaluation
- Inversion of Control (IoC) architecture: `deap_logic.py` + `ray_evaluator.py`
- Ordered `pool.map` replacing `map_unordered` (prevents freezes)
- GPU memory management with configurable fraction
- Comprehensive documentation

**Status**: âœ… Complete (February 2026)

### Phase 3: Testing & Validation (Q1 2026) ðŸš§ ONGOING
- [x] Core optimizer unit tests (62 tests) âœ…
- [x] Integration tests âœ…
- [x] Test documentation âœ…
- [ ] Ray parallel tests
- [ ] GA / DEAP tests
- [ ] CLI tests
- [ ] CI/CD pipeline
- [ ] Performance benchmarks

**Status**: ðŸš§ Core Tests Complete, Ray + GA Tests Pending  
**Started**: January 2026  
**Target**: February 2026

### Phase 4: Advanced Features (Q1-Q2 2026)
- Multi-objective optimization
- Mechanical reflector integration (initialization + control complete)
- Multi-AP optimization
- Adaptive learning rate
- Coarse-to-fine Ray-based search
- Hybrid GA+GD (seed GD from GA best solutions)

**Status**: ðŸš§ In progress (reflector control integration complete, advanced extensions pending)  
**Target**: March-April 2026

### Phase 5: Publishing & Release (Q2 2026)
- PyPI publication
- Documentation site
- Tutorial materials
- Video demonstrations
- v1.0.0 release

**Status**: ðŸ“‹ Planned  
**Target**: May 2026

## Documentation

### Main Guides
- **Quick Start**: See [Quick Start](#quick-start) section above
- **Installation**: See [docs/guides/INSTALL.md](docs/guides/INSTALL.md)
- **Usage Guide**: See [docs/guides/USAGE.md](docs/guides/USAGE.md)
- **Quick Reference**: See [docs/guides/QUICKREF.md](docs/guides/QUICKREF.md)
- **Ray Experiment Runner**: See [docs/guides/RAY_EXPERIMENT_RUNNER.md](docs/guides/RAY_EXPERIMENT_RUNNER.md)

### Architecture & Structure
- **Project Structure**: See [docs/architecture/PROJECT_STRUCTURE.md](docs/architecture/PROJECT_STRUCTURE.md)
- **Changelog**: See [docs/architecture/CHANGELOG.md](docs/architecture/CHANGELOG.md)

### Methodology & Research
- **Optimization Workflow**: See [docs/methodology/OPTIMIZATION_WORKFLOW.md](docs/methodology/OPTIMIZATION_WORKFLOW.md) - Ray-based distributed optimization architecture
- **Ray Parallel Guide**: See [docs/methodology/RAY_PARALLEL_GUIDE.md](docs/methodology/RAY_PARALLEL_GUIDE.md) - Complete guide to Ray parallel optimization
- **Ray Architecture**: See [docs/methodology/RAY_ARCHITECTURE.md](docs/methodology/RAY_ARCHITECTURE.md) - Why Ray vs vectorization
- **Ray Implementation**: See [docs/methodology/RAY_IMPLEMENTATION_SUMMARY.md](docs/methodology/RAY_IMPLEMENTATION_SUMMARY.md) - Implementation summary
- **Baseline Methods**: See [docs/methodology/BASELINES.md](docs/methodology/BASELINES.md) - GA, PSO, and AO comparisons
- **Future Roadmap**: See [docs/methodology/FUTURE_ROADMAP.md](docs/methodology/FUTURE_ROADMAP.md) - Advanced features and research extensions

### Testing & Quality
- **Test Hub**: See [docs/tests/README.md](docs/tests/README.md) - Complete testing documentation
- **Test Summary**: See [docs/tests/TEST_SUMMARY.md](docs/tests/TEST_SUMMARY.md) - Test statistics (62 tests, 82-92% coverage)
- **Testing Guide**: See [docs/tests/TESTING_GUIDE.md](docs/tests/TESTING_GUIDE.md) - How to run tests
- **Test Categories**: See [docs/tests/TEST_CATEGORIES.md](docs/tests/TEST_CATEGORIES.md) - Test organization

For a complete documentation index, see [docs/README.md](docs/README.md).


## Contributing

Contributions are welcome! Please:
1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

MIT License - see LICENSE file for details.

## Acknowledgments

This project uses:
- [Sionna](https://nvlabs.github.io/sionna/) for differentiable ray tracing
- [PyTorch](https://pytorch.org/) for gradient computation
- [DrJit](https://github.com/mitsuba-renderer/drjit) for PyTorch-Mitsuba integration

## Citation

If you use this code in your research, please cite:

```bibtex
@software{reflector_position,
  title = {Reflector Position Optimization},
  author = {Your Name},
  year = {2026},
  version = {0.1.0},
  url = {https://github.com/yourusername/reflector-position}
}
```

